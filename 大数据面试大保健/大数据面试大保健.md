# 大数据面试大保健

## 一、Linux

### 1、常见高级命令

```shell
Top
Iotop
Ps -ef：查看进程
Df -h：查看磁盘使用情况
Netstat：查看端口号
Jmap -heap：查看某个进程内存
Tar：解压
rpm
```



## 二、shell

### 1、用过哪些工具？

```shell
awk
sed
sort
cut

python
```



### 2、写过哪些shell脚本

```shell
1）分发脚本

2）启停脚本
	#! /bin/bash
	
	case $1 in
	"start")
		for i in hadoop102 hadoop103 hadoop104
		do
			ssh $i "命令"
		done
	;;
	"stop")
		for i in hadoop102 hadoop103 hadoop104
		do
			ssh $i "命令"
		done
	;;
	esac
3）与mysql的导入导出
	mysql ->(datax)-> hdfs
	hdfs ->(datax)-> mysql

4）数仓层级内部的数据传递
	ods -> ads
```



### 3、''（单引号）与""（双引号）的区别

​	' '：不解析里面变量的值

​	""：解析里面变量的值

​	嵌套：	看谁在最外面



## 三、Hadoop

### 	1、入门

#### 		1.1 常用端口号

###### 2.x

```shell
50070：HDFS页面访问端口

8088：Yarn任务调度访问端口

19888：历史服务器

9000/8020：内部通讯访问端口
```



###### 3.x

```shell
9870(有改变)：HDFS页面访问端口

8088：Yarn任务调度访问端口

19888：历史服务器

9820/9000/8020(有改变)：内部通讯访问端口
```



#### 		1.2 常用配置

###### 2.x

```shell
core-site.xml:
hdfs-site.xml:
mapred-site.xml:
yarn-site.xml:
slaves:
```



###### 3.x

```shell
core-site.xml:
hdfs-site.xml:
mapred-site.xml:
yarn-site.xml:
workers(有改变):
```

##### 	

### 2、HDFS

#### 		2.1 hdfs 读写流程 笔试题（有没有朋友）



#### 		2.2 小文件的危害

##### 	**2.2.1 存储 ：**

​		主要影响NameNode的存储，因为无论小文件多小，都会占用150字节存储。

​		文件块的计算：文件块数量 = 128G（内存）[128G / 1024(m) / 1024(kb) / 1024(字节)]/ 150字节 = 9.1亿

##### 	**2.2.2 计算**

​		默认的切片规则，每个文件单独切片。（1字节文件 -> 开启1个maptask -> 开启1G内存）



#### 2.3 小文件怎么解决

##### 	**2.3.1har归档**

​		将多个小文件包裹在一起，统一进行发送（类似发快递）

![image-20230221121640957](assets/image-20230221121640957.png)

​				

##### 	**2.3.2 CombineTextInputformat**

​		把所有文件放在一起统一切片



##### 	**2.3.3 JVM重用**

> ​	JVM重用是一把双刃剑，有小文件再开启。没有小文件，就会把一个任务执行非常长！

![image-20230221122613175](assets/image-20230221122613175.png)



#### 2.4 副本数 3个



#### 2.5 块大小

​	**块大小取决于硬盘的读写速度，普通的机械硬盘（100m/s）选择128m，普通固态硬盘（300m/s）选择250m，高级固态硬盘（600m/s）选择512m。**

> ​	1.x	64m	
>
> ​	2.x	128m
>
> ​	3.x	128m
>
> ​	本地	32m
>
> ​	企业	128m 256m 512m





​		

### 	3、MR



### 	4、Yarn

